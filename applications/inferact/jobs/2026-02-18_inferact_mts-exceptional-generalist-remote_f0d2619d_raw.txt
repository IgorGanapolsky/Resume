
    
      You need to enable JavaScript to run this app.
    
    Member of Technical Staff, Exceptional Generalist (Remote) LocationRemoteEmployment TypeFull timeLocation TypeRemoteDepartmentResearch & EngineeringOverviewApplicationInferact's mission is to grow vLLM as the world's AI inference engine and accelerate AI progress by making inference cheaper and faster. Founded by the creators and core maintainers of vLLM, we sit at the intersection of models and hardware—a position that took years to build.About the RoleThis is a globally remote opportunity. We're seeking exceptional generalist engineers who can work across the entire vLLM stack: from low-level GPU kernels to high-level distributed systems. This role is designed for self-directed, autonomous individuals who can identify the highest-leverage problems and solve them end-to-end without constant guidance.You'll work asynchronously with our San Francisco headquarters while maintaining full ownership of critical infrastructure. You might be optimizing CUDA kernels one week, designing distributed orchestration systems the next, and implementing new model architectures the week after. The work you do will directly impact how the world runs AI inference.Potential focus areas include:Inference Runtime: Push the boundaries of LLM and diffusion model serving. Work at the core of vLLM to optimize how models execute across diverse hardware and architectures.Kernel Engineering: Write the low-level kernels and optimizations that make vLLM the fastest inference engine in the world, running on hundreds of accelerator types.Performance & Scale: Build the distributed systems that power inference at global scale—design foundational layers enabling vLLM to serve models across thousands of accelerators with minimal latency.Cloud Orchestration: Build the operational backbone for cluster management, deployment automation, and production monitoring that enables teams worldwide to serve AI models without friction.What We're Looking ForWe're looking for engineers who thrive with autonomy. You should be able to take a vague problem statement and turn it into shipped code with minimal supervision. You communicate proactively, over-communicate context across time zones, and know when to ask for help versus when to push forward independently.Core Requirements:Bachelor's degree or equivalent experience in computer science, engineering, or similarDemonstrated ability to work autonomously and drive projects to completion without close supervisionExcellent asynchronous communication skills and ability to collaborate effectively across time zonesStrong track record of shipping high-impact work in complex technical environmentsDeep expertise in at least one of: systems programming, GPU/accelerator programming, distributed systems, or ML infrastructureTechnical Depth (strong in at least two):CUDA kernels or equivalent (Triton, TileLang, Pallas) with deep understanding of GPU architectureHigh-performance distributed systems in Rust, Go, or C++Python with PyTorch internals and LLM inference systems (vLLM, TensorRT-LLM, SGLang)Kubernetes, container orchestration, and infrastructure-as-code at scaleTransformer architectures, KV-cache memory management, and model servingPreferred Qualifications:Contributions to vLLM or other major open-source ML/systems projectsExperience with multiple accelerator platforms (NVIDIA, AMD, TPU, Intel)Knowledge of quantization techniques, ML-specific kernel optimization, or compiler technologiesTrack record of improving system reliability and performance at scaleWritten widely-shared technical blogs or impactful side projects in the ML infrastructure spaceLogisticsLocation: Fully remote, worldwide. We're timezone-flexible but expect regular overlap with Pacific Time for critical syncs.Compensation: We offer competitive compensations (salary + equity) compared to the local market conditions.Visa sponsorship: We sponsor visas on a case-by-case basis.Benefits: Inferact offers competitive benefits appropriate to your location, including health coverage where applicable.Apply for this JobThis site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply.Powered by AshbyPrivacy PolicySecurityVulnerability Disclosure
    
      <!-- [script/app-data removed by scrub_job_captures.py] -->,"showJobFilters":false,"showLocationAddress":false,"showTeams":false,"showAutofillApplicationsBox":true,"logoWordmarkImageUrl":"https://app.ashbyhq.com/api/images/org-theme-wordmark/cb39e13c-16ae-4596-8aee-e974aec364f9/ee55f709-c1f0-43c8-83bd-3a3b5883fa1c/858b094f-8a1a-4e09-80df-16f6eae58dad.png","logoSquareImageUrl":"https://app.ashbyhq.com/api/images/org-theme-logo/cb39e13c-16ae-4596-8aee-e974aec364f9/ee55f709-c1f0-43c8-83bd-3a3b5883fa1c/90c8cff4-f6bf-43e1-8dc5-28d787c97bdf.png","applicationSubmittedSuccessMessage":null,"jobBoardTopDescriptionHtml":null,"jobBoardBottomDescriptionHtml":null,"jobPostingBackUrl":null},"appConfirmationTrackingPixelHtml":null,"recruitingPrivacyPolicyUrl":null,"timezone":"America/Los_Angeles","candidateScheduleCancellationReasonRequirementStatus":"Off"},"posting":{"id":"f0d2619d-28e0-4b25-8d30-3ac555071abb","title":"Member of Technical Staff, Exceptional Generalist (Remote)","updatedAt":"2026-01-22T10:16:40.223Z","departmentName":"Research & Engineering","departmentExternalName":"Research & Engineering","teamName":"Research & Engineering","teamExternalName":"Research & Engineering","teamNames":["Research & Engineering"],"locationName":"Remote","locationExternalName":"Remote","isListed":true,"suppressDescriptionOpening":false,"suppressDescriptionClosing":false,"descriptionHtml":"<p style=\"min-height:1.5em\">Inferact's mission is to grow vLLM as the world's AI inference engine and accelerate AI progress by making inference cheaper and faster. Founded by the creators and core maintainers of vLLM, we sit at the intersection of models and hardware—a position that took years to build.</p><p style=\"min-height:1.5em\"></p><h2>About the Role</h2><p style=\"min-height:1.5em\">This is a globally remote opportunity. We're seeking <strong>exceptional generalist engineers</strong> who can work across the entire vLLM stack: from low-level GPU kernels to high-level distributed systems. This role is designed for self-directed, autonomous individuals who can identify the highest-leverage problems and solve them end-to-end without constant guidance.</p><p style=\"min-height:1.5em\">You'll work asynchronously with our San Francisco headquarters while maintaining full ownership of critical infrastructure. You might be optimizing CUDA kernels one week, designing distributed orchestration systems the next, and implementing new model architectures the week after. The work you do will directly impact how the world runs AI inference.</p><p style=\"min-height:1.5em\"><strong>Potential focus areas include:</strong></p><ul style=\"min-height:1.5em\"><li><p style=\"min-height:1.5em\"><strong>Inference Runtime:</strong> Push the boundaries of LLM and diffusion model serving. Work at the core of vLLM to optimize how models execute across diverse hardware and architectures.</p></li><li><p style=\"min-height:1.5em\"><strong>Kernel Engineering:</strong> Write the low-level kernels and optimizations that make vLLM the fastest inference engine in the world, running on hundreds of accelerator types.</p></li><li><p style=\"min-height:1.5em\"><strong>Performance &amp; Scale:</strong> Build the distributed systems that power inference at global scale—design foundational layers enabling vLLM to serve models across thousands of accelerators with minimal latency.</p></li><li><p style=\"min-height:1.5em\"><strong>Cloud Orchestration:</strong> Build the operational backbone for cluster management, deployment automation, and production monitoring that enables teams worldwide to serve AI models without friction.</p><p style=\"min-height:1.5em\"></p></li></ul><h2>What We're Looking For</h2><p style=\"min-height:1.5em\">We're looking for engineers<strong> who thrive with autonomy.</strong> You should be able to take a vague problem statement and turn it into shipped code with minimal supervision. You communicate proactively, over-communicate context across time zones, and know when to ask for help versus when to push forward independently.</p><p style=\"min-height:1.5em\"></p><p style=\"min-height:1.5em\"><strong>Core Requirements:</strong></p><ul style=\"min-height:1.5em\"><li><p style=\"min-height:1.5em\">Bachelor's degree or equivalent experience in computer science, engineering, or similar</p></li><li><p style=\"min-height:1.5em\">Demonstrated ability to work autonomously and drive projects to completion without close supervision</p></li><li><p style=\"min-height:1.5em\">Excellent asynchronous communication skills and ability to collaborate effectively across time zones</p></li><li><p style=\"min-height:1.5em\">Strong track record of shipping high-impact work in complex technical environments</p></li><li><p style=\"min-height:1.5em\">Deep expertise in at least one of: systems programming, GPU/accelerator programming, distributed systems, or ML infrastructure</p></li></ul><p style=\"min-height:1.5em\"><strong>Technical Depth (strong in at least two):</strong></p><ul style=\"min-height:1.5em\"><li><p style=\"min-height:1.5em\">CUDA kernels or equivalent (Triton, TileLang, Pallas) with deep understanding of GPU architecture</p></li><li><p style=\"min-height:1.5em\">High-performance distributed systems in Rust, Go, or C++</p></li><li><p style=\"min-height:1.5em\">Python with PyTorch internals and LLM inference systems (vLLM, TensorRT-LLM, SGLang)</p></li><li><p style=\"min-height:1.5em\">Kubernetes, container orchestration, and infrastructure-as-code at scale</p></li><li><p style=\"min-height:1.5em\">Transformer architectures, KV-cache memory management, and model serving</p></li></ul><p style=\"min-height:1.5em\"><strong>Preferred Qualifications:</strong></p><ul style=\"min-height:1.5em\"><li><p style=\"min-height:1.5em\">Contributions to vLLM or other major open-source ML/systems projects</p></li><li><p style=\"min-height:1.5em\">Experience with multiple accelerator platforms (NVIDIA, AMD, TPU, Intel)</p></li><li><p style=\"min-height:1.5em\">Knowledge of quantization techniques, ML-specific kernel optimization, or compiler technologies</p></li><li><p style=\"min-height:1.5em\">Track record of improving system reliability and performance at scale</p></li><li><p style=\"min-height:1.5em\">Written widely-shared technical blogs or impactful side projects in the ML infrastructure space</p><p style=\"min-height:1.5em\"></p></li></ul><h2>Logistics</h2><ul style=\"min-height:1.5em\"><li><p style=\"min-height:1.5em\"><strong>Location:</strong> Fully remote, worldwide. We're timezone-flexible but expect regular overlap with Pacific Time for critical syncs.</p></li><li><p style=\"min-height:1.5em\"><strong>Compensation:</strong> We offer competitive compensations (salary + equity) compared to the local market conditions.</p></li><li><p style=\"min-height:1.5em\"><strong>Visa sponsorship:</strong> We sponsor visas on a case-by-case basis.</p></li><li><p style=\"min-height:1.5em\"><strong>Benefits:</strong> Inferact offers competitive benefits appropriate to your location, including health coverage where applicable.</p></li></ul><p style=\"min-height:1.5em\"></p><p style=\"min-height:1.5em\"></p>","descriptionPlainText":"Inferact's mission is to grow vLLM as the world's AI inference engine and accelerate AI progress by making inference cheaper and faster. Founded by the creators and core maintainers of vLLM, we sit at the intersection of models and hardware—a position that took years to build.\n\n\nABOUT THE ROLE\n\nThis is a globally remote opportunity. We're seeking exceptional generalist engineers who can work across the entire vLLM stack: from low-level GPU kernels to high-level distributed systems. This role is designed for self-directed, autonomous individuals who can identify the highest-leverage problems and solve them end-to-end without constant guidance.\n\nYou'll work asynchronously with our San Francisco headquarters while maintaining full ownership of critical infrastructure. You might be optimizing CUDA kernels one week, designing distributed orchestration systems the next, and implementing new model architectures the week after. The work you do will directly impact how the world runs AI inference.\n\nPotential focus areas include:\n\n - Inference Runtime: Push the boundaries of LLM and diffusion model serving. Work at the core of vLLM to optimize how models execute across diverse hardware and architectures.\n\n - Kernel Engineering: Write the low-level kernels and optimizations that make vLLM the fastest inference engine in the world, running on hundreds of accelerator types.\n\n - Performance & Scale: Build the distributed systems that power inference at global scale—design foundational layers enabling vLLM to serve models across thousands of accelerators with minimal latency.\n\n - Cloud Orchestration: Build the operational backbone for cluster management, deployment automation, and production monitoring that enables teams worldwide to serve AI models without friction.\n\n\nWHAT WE'RE LOOKING FOR\n\nWe're looking for engineers who thrive with autonomy. You should be able to take a vague problem statement and turn it into shipped code with minimal supervision. You communicate proactively, over-communicate context across time zones, and know when to ask for help versus when to push forward independently.\n\nCore Requirements:\n\n - Bachelor's degree or equivalent experience in computer science, engineering, or similar\n\n - Demonstrated ability to work autonomously and drive projects to completion without close supervision\n\n - Excellent asynchronous communication skills and ability to collaborate effectively across time zones\n\n - Strong track record of shipping high-impact work in complex technical environments\n\n - Deep expertise in at least one of: systems programming, GPU/accelerator programming, distributed systems, or ML infrastructure\n\nTechnical Depth (strong in at least two):\n\n - CUDA kernels or equivalent (Triton, TileLang, Pallas) with deep understanding of GPU architecture\n\n - High-performance distributed systems in Rust, Go, or C++\n\n - Python with PyTorch internals and LLM inference systems (vLLM, TensorRT-LLM, SGLang)\n\n - Kubernetes, container orchestration, and infrastructure-as-code at scale\n\n - Transformer architectures, KV-cache memory management, and model serving\n\nPreferred Qualifications:\n\n - Contributions to vLLM or other major open-source ML/systems projects\n\n - Experience with multiple accelerator platforms (NVIDIA, AMD, TPU, Intel)\n\n - Knowledge of quantization techniques, ML-specific kernel optimization, or compiler technologies\n\n - Track record of improving system reliability and performance at scale\n\n - Written widely-shared technical blogs or impactful side projects in the ML infrastructure space\n\n\nLOGISTICS\n\n - Location: Fully remote, worldwide. We're timezone-flexible but expect regular overlap with Pacific Time for critical syncs.\n\n - Compensation: We offer competitive compensations (salary + equity) compared to the local market conditions.\n\n - Visa sponsorship: We sponsor visas on a case-by-case basis.\n\n - Benefits: Inferact offers competitive benefits appropriate to your location, including health coverage where applicable.","descriptionParts":{"descriptionOpening":null,"descriptionBody":{"html":"<p style=\"min-height:1.5em\">Inferact's mission is to grow vLLM as the world's AI inference engine and accelerate AI progress by making inference cheaper and faster. Founded by the creators and core maintainers of vLLM, we sit at the intersection of models and hardware—a position that took years to build.</p><p style=\"min-height:1.5em\"></p><h2>About the Role</h2><p style=\"min-height:1.5em\">This is a globally remote opportunity. We're seeking <strong>exceptional generalist engineers</strong> who can work across the entire vLLM stack: from low-level GPU kernels to high-level distributed systems. This role is designed for self-directed, autonomous individuals who can identify the highest-leverage problems and solve them end-to-end without constant guidance.</p><p style=\"min-height:1.5em\">You'll work asynchronously with our San Francisco headquarters while maintaining full ownership of critical infrastructure. You might be optimizing CUDA kernels one week, designing distributed orchestration systems the next, and implementing new model architectures the week after. The work you do will directly impact how the world runs AI inference.</p><p style=\"min-height:1.5em\"><strong>Potential focus areas include:</strong></p><ul style=\"min-height:1.5em\"><li><p style=\"min-height:1.5em\"><strong>Inference Runtime:</strong> Push the boundaries of LLM and diffusion model serving. Work at the core of vLLM to optimize how models execute across diverse hardware and architectures.</p></li><li><p style=\"min-height:1.5em\"><strong>Kernel Engineering:</strong> Write the low-level kernels and optimizations that make vLLM the fastest inference engine in the world, running on hundreds of accelerator types.</p></li><li><p style=\"min-height:1.5em\"><strong>Performance &amp; Scale:</strong> Build the distributed systems that power inference at global scale—design foundational layers enabling vLLM to serve models across thousands of accelerators with minimal latency.</p></li><li><p style=\"min-height:1.5em\"><strong>Cloud Orchestration:</strong> Build the operational backbone for cluster management, deployment automation, and production monitoring that enables teams worldwide to serve AI models without friction.</p><p style=\"min-height:1.5em\"></p></li></ul><h2>What We're Looking For</h2><p style=\"min-height:1.5em\">We're looking for engineers<strong> who thrive with autonomy.</strong> You should be able to take a vague problem statement and turn it into shipped code with minimal supervision. You communicate proactively, over-communicate context across time zones, and know when to ask for help versus when to push forward independently.</p><p style=\"min-height:1.5em\"></p><p style=\"min-height:1.5em\"><strong>Core Requirements:</strong></p><ul style=\"min-height:1.5em\"><li><p style=\"min-height:1.5em\">Bachelor's degree or equivalent experience in computer science, engineering, or similar</p></li><li><p style=\"min-height:1.5em\">Demonstrated ability to work autonomously and drive projects to completion without close supervision</p></li><li><p style=\"min-height:1.5em\">Excellent asynchronous communication skills and ability to collaborate effectively across time zones</p></li><li><p style=\"min-height:1.5em\">Strong track record of shipping high-impact work in complex technical environments</p></li><li><p style=\"min-height:1.5em\">Deep expertise in at least one of: systems programming, GPU/accelerator programming, distributed systems, or ML infrastructure</p></li></ul><p style=\"min-height:1.5em\"><strong>Technical Depth (strong in at least two):</strong></p><ul style=\"min-height:1.5em\"><li><p style=\"min-height:1.5em\">CUDA kernels or equivalent (Triton, TileLang, Pallas) with deep understanding of GPU architecture</p></li><li><p style=\"min-height:1.5em\">High-performance distributed systems in Rust, Go, or C++</p></li><li><p style=\"min-height:1.5em\">Python with PyTorch internals and LLM inference systems (vLLM, TensorRT-LLM, SGLang)</p></li><li><p style=\"min-height:1.5em\">Kubernetes, container orchestration, and infrastructure-as-code at scale</p></li><li><p style=\"min-height:1.5em\">Transformer architectures, KV-cache memory management, and model serving</p></li></ul><p style=\"min-height:1.5em\"><strong>Preferred Qualifications:</strong></p><ul style=\"min-height:1.5em\"><li><p style=\"min-height:1.5em\">Contributions to vLLM or other major open-source ML/systems projects</p></li><li><p style=\"min-height:1.5em\">Experience with multiple accelerator platforms (NVIDIA, AMD, TPU, Intel)</p></li><li><p style=\"min-height:1.5em\">Knowledge of quantization techniques, ML-specific kernel optimization, or compiler technologies</p></li><li><p style=\"min-height:1.5em\">Track record of improving system reliability and performance at scale</p></li><li><p style=\"min-height:1.5em\">Written widely-shared technical blogs or impactful side projects in the ML infrastructure space</p><p style=\"min-height:1.5em\"></p></li></ul><h2>Logistics</h2><ul style=\"min-height:1.5em\"><li><p style=\"min-height:1.5em\"><strong>Location:</strong> Fully remote, worldwide. We're timezone-flexible but expect regular overlap with Pacific Time for critical syncs.</p></li><li><p style=\"min-height:1.5em\"><strong>Compensation:</strong> We offer competitive compensations (salary + equity) compared to the local market conditions.</p></li><li><p style=\"min-height:1.5em\"><strong>Visa sponsorship:</strong> We sponsor visas on a case-by-case basis.</p></li><li><p style=\"min-height:1.5em\"><strong>Benefits:</strong> Inferact offers competitive benefits appropriate to your location, including health coverage where applicable.</p></li></ul><p style=\"min-height:1.5em\"></p><p style=\"min-height:1.5em\"></p>","plain":"Inferact's mission is to grow vLLM as the world's AI inference engine and accelerate AI progress by making inference cheaper and faster. Founded by the creators and core maintainers of vLLM, we sit at the intersection of models and hardware—a position that took years to build.\n\n\nABOUT THE ROLE\n\nThis is a globally remote opportunity. We're seeking exceptional generalist engineers who can work across the entire vLLM stack: from low-level GPU kernels to high-level distributed systems. This role is designed for self-directed, autonomous individuals who can identify the highest-leverage problems and solve them end-to-end without constant guidance.\n\nYou'll work asynchronously with our San Francisco headquarters while maintaining full ownership of critical infrastructure. You might be optimizing CUDA kernels one week, designing distributed orchestration systems the next, and implementing new model architectures the week after. The work you do will directly impact how the world runs AI inference.\n\nPotential focus areas include:\n\n - Inference Runtime: Push the boundaries of LLM and diffusion model serving. Work at the core of vLLM to optimize how models execute across diverse hardware and architectures.\n\n - Kernel Engineering: Write the low-level kernels and optimizations that make vLLM the fastest inference engine in the world, running on hundreds of accelerator types.\n\n - Performance & Scale: Build the distributed systems that power inference at global scale—design foundational layers enabling vLLM to serve models across thousands of accelerators with minimal latency.\n\n - Cloud Orchestration: Build the operational backbone for cluster management, deployment automation, and production monitoring that enables teams worldwide to serve AI models without friction.\n\n\nWHAT WE'RE LOOKING FOR\n\nWe're looking for engineers who thrive with autonomy. You should be able to take a vague problem statement and turn it into shipped code with minimal supervision. You communicate proactively, over-communicate context across time zones, and know when to ask for help versus when to push forward independently.\n\nCore Requirements:\n\n - Bachelor's degree or equivalent experience in computer science, engineering, or similar\n\n - Demonstrated ability to work autonomously and drive projects to completion without close supervision\n\n - Excellent asynchronous communication skills and ability to collaborate effectively across time zones\n\n - Strong track record of shipping high-impact work in complex technical environments\n\n - Deep expertise in at least one of: systems programming, GPU/accelerator programming, distributed systems, or ML infrastructure\n\nTechnical Depth (strong in at least two):\n\n - CUDA kernels or equivalent (Triton, TileLang, Pallas) with deep understanding of GPU architecture\n\n - High-performance distributed systems in Rust, Go, or C++\n\n - Python with PyTorch internals and LLM inference systems (vLLM, TensorRT-LLM, SGLang)\n\n - Kubernetes, container orchestration, and infrastructure-as-code at scale\n\n - Transformer architectures, KV-cache memory management, and model serving\n\nPreferred Qualifications:\n\n - Contributions to vLLM or other major open-source ML/systems projects\n\n - Experience with multiple accelerator platforms (NVIDIA, AMD, TPU, Intel)\n\n - Knowledge of quantization techniques, ML-specific kernel optimization, or compiler technologies\n\n - Track record of improving system reliability and performance at scale\n\n - Written widely-shared technical blogs or impactful side projects in the ML infrastructure space\n\n\nLOGISTICS\n\n - Location: Fully remote, worldwide. We're timezone-flexible but expect regular overlap with Pacific Time for critical syncs.\n\n - Compensation: We offer competitive compensations (salary + equity) compared to the local market conditions.\n\n - Visa sponsorship: We sponsor visas on a case-by-case basis.\n\n - Benefits: Inferact offers competitive benefits appropriate to your location, including health coverage where applicable."},"descriptionClosing":null},"shortDescription":"Inferact's mission is to grow vLLM as the world's AI inference engine and accelerate AI progress by making inference cheaper and faster. Founded by the creators and core maintainers of vLLM, we sit at the intersection of models and hardware—a position that took years to build.","applicationFormDefinitionId":"0a41a888-0a37-466a-8678-be79c2f2118b","surveyFormDefinitionIds":[],"linkedData":{"@context":"https://schema.org/","@type":"JobPosting","title":"Member of Technical Staff, Exceptional Generalist (Remote)","description":"<p style=\"min-height:1.5em\">Inferact's mission is to grow vLLM as the world's AI inference engine and accelerate AI progress by making inference cheaper and faster. Founded by the creators and core maintainers of vLLM, we sit at the intersection of models and hardware—a position that took years to build.</p><p style=\"min-height:1.5em\"></p><h2>About the Role</h2><p style=\"min-height:1.5em\">This is a globally remote opportunity. We're seeking <strong>exceptional generalist engineers</strong> who can work across the entire vLLM stack: from low-level GPU kernels to high-level distributed systems. This role is designed for self-directed, autonomous individuals who can identify the highest-leverage problems and solve them end-to-end without constant guidance.</p><p style=\"min-height:1.5em\">You'll work asynchronously with our San Francisco headquarters while maintaining full ownership of critical infrastructure. You might be optimizing CUDA kernels one week, designing distributed orchestration systems the next, and implementing new model architectures the week after. The work you do will directly impact how the world runs AI inference.</p><p style=\"min-height:1.5em\"><strong>Potential focus areas include:</strong></p><ul style=\"min-height:1.5em\"><li><p style=\"min-height:1.5em\"><strong>Inference Runtime:</strong> Push the boundaries of LLM and diffusion model serving. Work at the core of vLLM to optimize how models execute across diverse hardware and architectures.</p></li><li><p style=\"min-height:1.5em\"><strong>Kernel Engineering:</strong> Write the low-level kernels and optimizations that make vLLM the fastest inference engine in the world, running on hundreds of accelerator types.</p></li><li><p style=\"min-height:1.5em\"><strong>Performance &amp; Scale:</strong> Build the distributed systems that power inference at global scale—design foundational layers enabling vLLM to serve models across thousands of accelerators with minimal latency.</p></li><li><p style=\"min-height:1.5em\"><strong>Cloud Orchestration:</strong> Build the operational backbone for cluster management, deployment automation, and production monitoring that enables teams worldwide to serve AI models without friction.</p><p style=\"min-height:1.5em\"></p></li></ul><h2>What We're Looking For</h2><p style=\"min-height:1.5em\">We're looking for engineers<strong> who thrive with autonomy.</strong> You should be able to take a vague problem statement and turn it into shipped code with minimal supervision. You communicate proactively, over-communicate context across time zones, and know when to ask for help versus when to push forward independently.</p><p style=\"min-height:1.5em\"></p><p style=\"min-height:1.5em\"><strong>Core Requirements:</strong></p><ul style=\"min-height:1.5em\"><li><p style=\"min-height:1.5em\">Bachelor's degree or equivalent experience in computer science, engineering, or similar</p></li><li><p style=\"min-height:1.5em\">Demonstrated ability to work autonomously and drive projects to completion without close supervision</p></li><li><p style=\"min-height:1.5em\">Excellent asynchronous communication skills and ability to collaborate effectively across time zones</p></li><li><p style=\"min-height:1.5em\">Strong track record of shipping high-impact work in complex technical environments</p></li><li><p style=\"min-height:1.5em\">Deep expertise in at least one of: systems programming, GPU/accelerator programming, distributed systems, or ML infrastructure</p></li></ul><p style=\"min-height:1.5em\"><strong>Technical Depth (strong in at least two):</strong></p><ul style=\"min-height:1.5em\"><li><p style=\"min-height:1.5em\">CUDA kernels or equivalent (Triton, TileLang, Pallas) with deep understanding of GPU architecture</p></li><li><p style=\"min-height:1.5em\">High-performance distributed systems in Rust, Go, or C++</p></li><li><p style=\"min-height:1.5em\">Python with PyTorch internals and LLM inference systems (vLLM, TensorRT-LLM, SGLang)</p></li><li><p style=\"min-height:1.5em\">Kubernetes, container orchestration, and infrastructure-as-code at scale</p></li><li><p style=\"min-height:1.5em\">Transformer architectures, KV-cache memory management, and model serving</p></li></ul><p style=\"min-height:1.5em\"><strong>Preferred Qualifications:</strong></p><ul style=\"min-height:1.5em\"><li><p style=\"min-height:1.5em\">Contributions to vLLM or other major open-source ML/systems projects</p></li><li><p style=\"min-height:1.5em\">Experience with multiple accelerator platforms (NVIDIA, AMD, TPU, Intel)</p></li><li><p style=\"min-height:1.5em\">Knowledge of quantization techniques, ML-specific kernel optimization, or compiler technologies</p></li><li><p style=\"min-height:1.5em\">Track record of improving system reliability and performance at scale</p></li><li><p style=\"min-height:1.5em\">Written widely-shared technical blogs or impactful side projects in the ML infrastructure space</p><p style=\"min-height:1.5em\"></p></li></ul><h2>Logistics</h2><ul style=\"min-height:1.5em\"><li><p style=\"min-height:1.5em\"><strong>Location:</strong> Fully remote, worldwide. We're timezone-flexible but expect regular overlap with Pacific Time for critical syncs.</p></li><li><p style=\"min-height:1.5em\"><strong>Compensation:</strong> We offer competitive compensations (salary + equity) compared to the local market conditions.</p></li><li><p style=\"min-height:1.5em\"><strong>Visa sponsorship:</strong> We sponsor visas on a case-by-case basis.</p></li><li><p style=\"min-height:1.5em\"><strong>Benefits:</strong> Inferact offers competitive benefits appropriate to your location, including health coverage where applicable.</p></li></ul><p style=\"min-height:1.5em\"></p><p style=\"min-height:1.5em\"></p>","directApply":false,"identifier":{"@type":"PropertyValue","name":"Inferact","value":"f0d2619d-28e0-4b25-8d30-3ac555071abb"},"datePosted":"2026-01-22","hiringOrganization":{"@type":"Organization","name":"Inferact","sameAs":"https://inferact.ai/","logo":"https://app.ashbyhq.com/api/images/org-theme-logo/cb39e13c-16ae-4596-8aee-e974aec364f9/ee55f709-c1f0-43c8-83bd-3a3b5883fa1c/90c8cff4-f6bf-43e1-8dc5-28d787c97bdf.png"},"jobLocation":{"@type":"Place","address":{"@type":"PostalAddress","addressCountry":"US"}},"employmentType":"FULL_TIME","jobLocationType":"TELECOMMUTE","applicantLocationRequirements":{"@type":"Country","name":"US"}},"publishedDate":"2026-01-22","applicationDeadline":null,"structuredMetadataOverride":null,"address":{"postalAddress":{"addressCountry":"US"}},"isRemote":true,"workplaceType":"Remote","employmentType":"FullTime","jobId":"076e9993-db48-456c-afbc-408c89ced011","jobRequisitionId":null,"organizationId":"cb39e13c-16ae-4596-8aee-e974aec364f9","isConfidential":false,"compensationTierSummary":null,"summaryComponents":[],"compensationTiers":[],"compensationTierGuideUrl":null,"compensationPhilosophyHtml":null,"compensationPhilosophyPlainText":null,"scrapeableCompensationSalarySummary":null,"shouldDisplayCompensation":false,"shouldAskForTextingConsent":false,"legalEntityNameForTextingConsent":"Inferact","userRoles":[],"applicationForm":{"fieldEntries":[{"id":"71be59b6-34d3-488a-91b7-5fbffe33c0ab__systemfield_name","field":{"id":"30312d07-8698-47d4-b80f-e53bf403130d","path":"_systemfield_name","humanReadablePath":"Name","title":"Name","isNullable":false,"isPrivate":false,"isDeactivated":false,"isMany":false,"metadata":{},"type":"String","__autoSerializationID":"StringField"},"isRequired":true,"privacy":"default"},{"id":"71be59b6-34d3-488a-91b7-5fbffe33c0ab__systemfield_email","field":{"id":"3a1c2a46-c3f2-45df-9ec1-cbfa044c24cf","path":"_systemfield_email","humanReadablePath":"Email","title":"Email","isNullable":false,"isPrivate":false,"isDeactivated":false,"isMany":false,"metadata":{},"type":"Email","__autoSerializationID":"EmailField"},"isRequired":true,"privacy":"default"},{"id":"71be59b6-34d3-488a-91b7-5fbffe33c0ab_1a090d3e-5126-45b5-8b4c-20743fcf91e6","field":{"id":"1a090d3e-5126-45b5-8b4c-20743fcf91e6","path":"1a090d3e-5126-45b5-8b4c-20743fcf91e6","humanReadablePath":"","title":"GitHub Handle","isNullable":false,"isPrivate":false,"isDeactivated":false,"isMany":false,"metadata":{},"type":"String","__autoSerializationID":"StringField"},"isRequired":true,"privacy":"default"},{"id":"71be59b6-34d3-488a-91b7-5fbffe33c0ab__systemfield_resume","field":{"id":"6a17f205-c83e-4472-a1a2-a4537cd03b93","path":"_systemfield_resume","humanReadablePath":"Resume","title":"Resume","isNullable":false,"isPrivate":false,"isDeactivated":false,"isMany":false,"metadata":{},"type":"File","__autoSerializationID":"FileField"},"isRequired":true,"privacy":"default"},{"id":"71be59b6-34d3-488a-91b7-5fbffe33c0ab__systemfield_location","field":{"id":"3fb0aedc-5ac7-41bc-82b5-de8b4a2009e8","path":"_systemfield_location","humanReadablePath":"Location","title":"Location","isNullable":false,"isPrivate":false,"isDeactivated":false,"isMany":false,"metadata":{},"type":"Location","locationTypes":["Country"],"__autoSerializationID":"LocationField"},"isRequired":true,"privacy":"default"},{"id":"71be59b6-34d3-488a-91b7-5fbffe33c0ab_d52e3128-670e-4974-a6b5-ed95d600b855","field":{"id":"ba44f256-8b08-43e0-a6a7-efb1ee6d7fd4","path":"d52e3128-670e-4974-a6b5-ed95d600b855","humanReadablePath":"","title":"Please share a link to a personal project, open-source contribution, or technical blog post that you're most proud of or find most relevant to this role.","isNullable":false,"isPrivate":false,"isDeactivated":false,"isMany":false,"metadata":{},"type":"LongText","__autoSerializationID":"LongTextField"},"isRequired":true,"privacy":"default"},{"id":"71be59b6-34d3-488a-91b7-5fbffe33c0ab_edb57d28-3daf-4b27-a085-ac5d87ed033a","field":{"id":"468b36c5-bd09-48a1-ae53-092dc6c0db10","path":"edb57d28-3daf-4b27-a085-ac5d87ed033a","humanReadablePath":"","title":"We are hiring exceptional system/infrastructure engineers for this position - please select domains of infrastructure  you have expertise in.","isNullable":false,"isPrivate":false,"isDeactivated":false,"isMany":false,"metadata":{},"type":"MultiValueSelect","selectableValues":[{"label":"Inference Runtime","value":"Inference Runtime"},{"label":"Kernel Programming","value":"Kernel Programming","isArchived":false},{"label":"ML Compilers","value":"ML Compilers","isArchived":false},{"label":"High-performance Computing","value":"High-performance Computing","isArchived":false},{"label":"RL System","value":"RL System","isArchived":false},{"label":"Cloud Orchestration","value":"Cloud Orchestration","isArchived":false},{"label":"Other","value":"Other","isArchived":false}],"__autoSerializationID":"MultiValueSelectField"},"isRequired":true,"privacy":"default"},{"id":"71be59b6-34d3-488a-91b7-5fbffe33c0ab_f1661457-7ad3-422e-b6c0-2fb75bfb8a1f","field":{"id":"5276a78c-19f7-4550-9ae7-f3387830d7f4","path":"f1661457-7ad3-422e-b6c0-2fb75bfb8a1f","humanReadablePath":"","title":"If you selected \"other\", what areas did we not include that you have expertise in?","isNullable":false,"isPrivate":false,"isDeactivated":false,"isMany":false,"metadata":{},"type":"LongText","__autoSerializationID":"LongTextField"},"isRequired":false,"privacy":"default"}],"sections":[{"fieldEntries":[{"id":"71be59b6-34d3-488a-91b7-5fbffe33c0ab__systemfield_name","field":{"id":"30312d07-8698-47d4-b80f-e53bf403130d","path":"_systemfield_name","humanReadablePath":"Name","title":"Name","isNullable":false,"isPrivate":false,"isDeactivated":false,"isMany":false,"metadata":{},"type":"String","__autoSerializationID":"StringField"},"isRequired":true,"privacy":"default"},{"id":"71be59b6-34d3-488a-91b7-5fbffe33c0ab__systemfield_email","field":{"id":"3a1c2a46-c3f2-45df-9ec1-cbfa044c24cf","path":"_systemfield_email","humanReadablePath":"Email","title":"Email","isNullable":false,"isPrivate":false,"isDeactivated":false,"isMany":false,"metadata":{},"type":"Email","__autoSerializationID":"EmailField"},"isRequired":true,"privacy":"default"},{"id":"71be59b6-34d3-488a-91b7-5fbffe33c0ab_1a090d3e-5126-45b5-8b4c-20743fcf91e6","field":{"id":"1a090d3e-5126-45b5-8b4c-20743fcf91e6","path":"1a090d3e-5126-45b5-8b4c-20743fcf91e6","humanReadablePath":"","title":"GitHub Handle","isNullable":false,"isPrivate":false,"isDeactivated":false,"isMany":false,"metadata":{},"type":"String","__autoSerializationID":"StringField"},"isRequired":true,"privacy":"default"},{"id":"71be59b6-34d3-488a-91b7-5fbffe33c0ab__systemfield_resume","field":{"id":"6a17f205-c83e-4472-a1a2-a4537cd03b93","path":"_systemfield_resume","humanReadablePath":"Resume","title":"Resume","isNullable":false,"isPrivate":false,"isDeactivated":false,"isMany":false,"metadata":{},"type":"File","__autoSerializationID":"FileField"},"isRequired":true,"privacy":"default"},{"id":"71be59b6-34d3-488a-91b7-5fbffe33c0ab__systemfield_location","field":{"id":"3fb0aedc-5ac7-41bc-82b5-de8b4a2009e8","path":"_systemfield_location","humanReadablePath":"Location","title":"Location","isNullable":false,"isPrivate":false,"isDeactivated":false,"isMany":false,"metadata":{},"type":"Location","locationTypes":["Country"],"__autoSerializationID":"LocationField"},"isRequired":true,"privacy":"default"},{"id":"71be59b6-34d3-488a-91b7-5fbffe33c0ab_d52e3128-670e-4974-a6b5-ed95d600b855","field":{"id":"ba44f256-8b08-43e0-a6a7-efb1ee6d7fd4","path":"d52e3128-670e-4974-a6b5-ed95d600b855","humanReadablePath":"","title":"Please share a link to a personal project, open-source contribution, or technical blog post that you're most proud of or find most relevant to this role.","isNullable":false,"isPrivate":false,"isDeactivated":false,"isMany":false,"metadata":{},"type":"LongText","__autoSerializationID":"LongTextField"},"isRequired":true,"privacy":"default"},{"id":"71be59b6-34d3-488a-91b7-5fbffe33c0ab_edb57d28-3daf-4b27-a085-ac5d87ed033a","field":{"id":"468b36c5-bd09-48a1-ae53-092dc6c0db10","path":"edb57d28-3daf-4b27-a085-ac5d87ed033a","humanReadablePath":"","title":"We are hiring exceptional system/infrastructure engineers for this position - please select domains of infrastructure  you have expertise in.","isNullable":false,"isPrivate":false,"isDeactivated":false,"isMany":false,"metadata":{},"type":"MultiValueSelect","selectableValues":[{"label":"Inference Runtime","value":"Inference Runtime"},{"label":"Kernel Programming","value":"Kernel Programming","isArchived":false},{"label":"ML Compilers","value":"ML Compilers","isArchived":false},{"label":"High-performance Computing","value":"High-performance Computing","isArchived":false},{"label":"RL System","value":"RL System","isArchived":false},{"label":"Cloud Orchestration","value":"Cloud Orchestration","isArchived":false},{"label":"Other","value":"Other","isArchived":false}],"__autoSerializationID":"MultiValueSelectField"},"isRequired":true,"privacy":"default"},{"id":"71be59b6-34d3-488a-91b7-5fbffe33c0ab_f1661457-7ad3-422e-b6c0-2fb75bfb8a1f","field":{"id":"5276a78c-19f7-4550-9ae7-f3387830d7f4","path":"f1661457-7ad3-422e-b6c0-2fb75bfb8a1f","humanReadablePath":"","title":"If you selected \"other\", what areas did we not include that you have expertise in?","isNullable":false,"isPrivate":false,"isDeactivated":false,"isMany":false,"metadata":{},"type":"LongText","__autoSerializationID":"LongTextField"},"isRequired":false,"privacy":"default"}]}],"formControls":[{"identifier":"a6d664e8-29bc-4fc5-b7a0-ecedd4a1cea6","title":"Submit","__autoSerializationID":"FormSubmit"}],"errorMessagesByFieldPath":{},"hiddenFieldPathsWithoutDefaults":[],"id":"71be59b6-34d3-488a-91b7-5fbffe33c0ab","entries":[{"field":{"id":"30312d07-8698-47d4-b80f-e53bf403130d","path":"_systemfield_name","humanReadablePath":"Name","title":"Name","isNullable":false,"isPrivate":false,"isDeactivated":false,"isMany":false,"metadata":{},"type":"String","__autoSerializationID":"StringField"},"privacy":"default","isRequired":true,"isRemoveable":false,"__autoSerializationID":"FormFieldDefinition"},{"field":{"id":"3a1c2a46-c3f2-45df-9ec1-cbfa044c24cf","path":"_systemfield_email","humanReadablePath":"Email","title":"Email","isNullable":false,"isPrivate":false,"isDeactivated":false,"isMany":false,"metadata":{},"type":"Email","__autoSerializationID":"EmailField"},"privacy":"default","isRequired":true,"connectedPath":"candidate.primary_personal_email_address","isRemoveable":false,"__autoSerializationID":"FormFieldDefinition"},{"field":{"id":"1a090d3e-5126-45b5-8b4c-20743fcf91e6","path":"1a090d3e-5126-45b5-8b4c-20743fcf91e6","humanReadablePath":"","title":"GitHub Handle","isNullable":false,"isPrivate":false,"isDeactivated":false,"isMany":false,"metadata":{},"type":"String","__autoSerializationID":"StringField"},"privacy":"default","isRequired":true,"isRemoveable":true,"__autoSerializationID":"FormFieldDefinition"},{"field":{"id":"6a17f205-c83e-4472-a1a2-a4537cd03b93","path":"_systemfield_resume","humanReadablePath":"Resume","title":"Resume","isNullable":false,"isPrivate":false,"isDeactivated":false,"isMany":false,"metadata":{},"type":"File","__autoSerializationID":"FileField"},"privacy":"default","isRequired":true,"isRemoveable":true,"__autoSerializationID":"FormFieldDefinition"},{"field":{"id":"3fb0aedc-5ac7-41bc-82b5-de8b4a2009e8","path":"_systemfield_location","humanReadablePath":"Location","title":"Location","isNullable":false,"isPrivate":false,"isDeactivated":false,"isMany":false,"metadata":{},"type":"Location","locationTypes":["Country"],"__autoSerializationID":"LocationField"},"privacy":"default","isRequired":true,"isRemoveable":true,"__autoSerializationID":"FormFieldDefinition"},{"field":{"id":"ba44f256-8b08-43e0-a6a7-efb1ee6d7fd4","path":"d52e3128-670e-4974-a6b5-ed95d600b855","humanReadablePath":"","title":"Please share a link to a personal project, open-source contribution, or technical blog post that you're most proud of or find most relevant to this role.","isNullable":false,"isPrivate":false,"isDeactivated":false,"isMany":false,"metadata":{},"type":"LongText","__autoSerializationID":"LongTextField"},"privacy":"default","isRequired":true,"isRemoveable":true,"__autoSerializationID":"FormFieldDefinition"},{"field":{"id":"468b36c5-bd09-48a1-ae53-092dc6c0db10","path":"edb57d28-3daf-4b27-a085-ac5d87ed033a","humanReadablePath":"","title":"We are hiring exceptional system/infrastructure engineers for this position - please select domains of infrastructure  you have expertise in.","isNullable":false,"isPrivate":false,"isDeactivated":false,"isMany":false,"metadata":{},"type":"MultiValueSelect","selectableValues":[{"label":"Inference Runtime","value":"Inference Runtime"},{"label":"Kernel Programming","value":"Kernel Programming","isArchived":false},{"label":"ML Compilers","value":"ML Compilers","isArchived":false},{"label":"High-performance Computing","value":"High-performance Computing","isArchived":false},{"label":"RL System","value":"RL System","isArchived":false},{"label":"Cloud Orchestration","value":"Cloud Orchestration","isArchived":false},{"label":"Other","value":"Other","isArchived":false}],"__autoSerializationID":"MultiValueSelectField"},"privacy":"default","isRequired":true,"isRemoveable":true,"__autoSerializationID":"FormFieldDefinition"},{"field":{"id":"5276a78c-19f7-4550-9ae7-f3387830d7f4","path":"f1661457-7ad3-422e-b6c0-2fb75bfb8a1f","humanReadablePath":"","title":"If you selected \"other\", what areas did we not include that you have expertise in?","isNullable":false,"isPrivate":false,"isDeactivated":false,"isMany":false,"metadata":{},"type":"LongText","__autoSerializationID":"LongTextField"},"privacy":"default","isRequired":false,"isRemoveable":true,"__autoSerializationID":"FormFieldDefinition"},{"identifier":"a6d664e8-29bc-4fc5-b7a0-ecedd4a1cea6","title":"Submit","__autoSerializationID":"FormSubmit"}],"formInstanceId":"99b5f274-3e01-4fce-a137-3b1bbdd0ef77","formDefinition":{"persistResponses":true,"sections":[{"fields":[{"field":{"id":"30312d07-8698-47d4-b80f-e53bf403130d","path":"_systemfield_name","humanReadablePath":"Name","title":"Name","isNullable":false,"isPrivate":false,"isDeactivated":false,"isMany":false,"metadata":{},"type":"String","__autoSerializationID":"StringField"},"privacy":"default","isRequired":true,"isRemoveable":false,"__autoSerializationID":"FormFieldDefinition"},{"field":{"id":"3a1c2a46-c3f2-45df-9ec1-cbfa044c24cf","path":"_systemfield_email","humanReadablePath":"Email","title":"Email","isNullable":false,"isPrivate":false,"isDeactivated":false,"isMany":false,"metadata":{},"type":"Email","__autoSerializationID":"EmailField"},"privacy":"default","isRequired":true,"connectedPath":"candidate.primary_personal_email_address","isRemoveable":false,"__autoSerializationID":"FormFieldDefinition"},{"field":{"id":"1a090d3e-5126-45b5-8b4c-20743fcf91e6","path":"1a090d3e-5126-45b5-8b4c-20743fcf91e6","humanReadablePath":"","title":"GitHub Handle","isNullable":false,"isPrivate":false,"isDeactivated":false,"isMany":false,"metadata":{},"type":"String","__autoSerializationID":"StringField"},"privacy":"default","isRequired":true,"isRemoveable":true,"__autoSerializationID":"FormFieldDefinition"},{"field":{"id":"6a17f205-c83e-4472-a1a2-a4537cd03b93","path":"_systemfield_resume","humanReadablePath":"Resume","title":"Resume","isNullable":false,"isPrivate":false,"isDeactivated":false,"isMany":false,"metadata":{},"type":"File","__autoSerializationID":"FileField"},"privacy":"default","isRequired":true,"isRemoveable":true,"__autoSerializationID":"FormFieldDefinition"},{"field":{"id":"3fb0aedc-5ac7-41bc-82b5-de8b4a2009e8","path":"_systemfield_location","humanReadablePath":"Location","title":"Location","isNullable":false,"isPrivate":false,"isDeactivated":false,"isMany":false,"metadata":{},"type":"Location","locationTypes":["Country"],"__autoSerializationID":"LocationField"},"privacy":"default","isRequired":true,"isRemoveable":true,"__autoSerializationID":"FormFieldDefinition"},{"field":{"id":"ba44f256-8b08-43e0-a6a7-efb1ee6d7fd4","path":"d52e3128-670e-4974-a6b5-ed95d600b855","humanReadablePath":"","title":"Please share a link to a personal project, open-source contribution, or technical blog post that you're most proud of or find most relevant to this role.","isNullable":false,"isPrivate":false,"isDeactivated":false,"isMany":false,"metadata":{},"type":"LongText","__autoSerializationID":"LongTextField"},"privacy":"default","isRequired":true,"isRemoveable":true,"__autoSerializationID":"FormFieldDefinition"},{"field":{"id":"468b36c5-bd09-48a1-ae53-092dc6c0db10","path":"edb57d28-3daf-4b27-a085-ac5d87ed033a","humanReadablePath":"","title":"We are hiring exceptional system/infrastructure engineers for this position - please select domains of infrastructure  you have expertise in.","isNullable":false,"isPrivate":false,"isDeactivated":false,"isMany":false,"metadata":{},"type":"MultiValueSelect","selectableValues":[{"label":"Inference Runtime","value":"Inference Runtime"},{"label":"Kernel Programming","value":"Kernel Programming","isArchived":false},{"label":"ML Compilers","value":"ML Compilers","isArchived":false},{"label":"High-performance Computing","value":"High-performance Computing","isArchived":false},{"label":"RL System","value":"RL System","isArchived":false},{"label":"Cloud Orchestration","value":"Cloud Orchestration","isArchived":false},{"label":"Other","value":"Other","isArchived":false}],"__autoSerializationID":"MultiValueSelectField"},"privacy":"default","isRequired":true,"isRemoveable":true,"__autoSerializationID":"FormFieldDefinition"},{"field":{"id":"5276a78c-19f7-4550-9ae7-f3387830d7f4","path":"f1661457-7ad3-422e-b6c0-2fb75bfb8a1f","humanReadablePath":"","title":"If you selected \"other\", what areas did we not include that you have expertise in?","isNullable":false,"isPrivate":false,"isDeactivated":false,"isMany":false,"metadata":{},"type":"LongText","__autoSerializationID":"LongTextField"},"privacy":"default","isRequired":false,"isRemoveable":true,"__autoSerializationID":"FormFieldDefinition"}],"__autoSerializationID":"FormSection"}],"linkedEntities":[],"__autoSerializationID":"FormDefinition"},"errorMessages":[],"submittedValues":{},"sourceFormDefinitionId":"0a41a888-0a37-466a-8678-be79c2f2118b","fieldPathDefaultObjectProxyMap":{},"formType":"None","staticLinkedEntities":[],"__autoSerializationID":"FormRender"},"surveyForms":[],"secondaryLocationNames":[]},"jobBoard":null,"routerPrefix":"/","recaptchaPublicSiteKey":"6LeFb_YUAAAAALUD5h-BiQEp8JaFChe0e0A6r49Y"};
      fetch("https://cdn.ashbyprd.com/frontend_non_user/f6b7eaaca2311d25c67d45228fd50830285910b7/.vite/manifest.json").then(function (res) { return res.json() }).then(function (manifest) {
        const indexData = manifest["index.html"];
    
        let bundleLoaded = false;
        function loadBundle() {
          if (bundleLoaded === true) {
            return;
          }
    
          const el = document.createElement("script");
          el.setAttribute("type", "module");
          el.setAttribute("crossorigin", "");
          el.setAttribute("integrity", indexData.integrity);
          el.setAttribute("src", "https://cdn.ashbyprd.com/frontend_non_user/f6b7eaaca2311d25c67d45228fd50830285910b7/" + indexData.file);
          document.head.appendChild(el);
          bundleLoaded = true;
        }
    
        if (indexData.css != null && indexData.css.length > 0) {
          const loadedSheets = [];
          indexData.css.forEach(function (sheet) {
            const link = document.createElement("link");
            link.rel = "stylesheet";
            link.type = "text/css";
            link.href = "https://cdn.ashbyprd.com/frontend_non_user/f6b7eaaca2311d25c67d45228fd50830285910b7/" + sheet;
            link.media = "all";
            link.onload = function () {
              loadedSheets.push(sheet);
              if (loadedSheets.length === indexData.css.length) {
                loadBundle();
              }
            };
            link.onerror = loadBundle;
            document.head.insertBefore(link, document.getElementById("vite-preload"));
          });
          const preload = document.createElement("link");
          preload.rel = "modulepreload";
          preload.href = "https://cdn.ashbyprd.com/frontend_non_user/f6b7eaaca2311d25c67d45228fd50830285910b7/" + indexData.file;
          document.head.appendChild(preload);
        } else {
          loadBundle();
        }
    
        if (indexData.imports != null && indexData.imports.length > 0) {
          indexData.imports.forEach(function (file) {
            const preload = document.createElement("link");
            preload.rel = "modulepreload";
            preload.href = "https://cdn.ashbyprd.com/frontend_non_user/f6b7eaaca2311d25c67d45228fd50830285910b7/" + manifest[file].file;
            document.head.appendChild(preload);
          });
        }
      });
      

